{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import math\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Compose, Normalize,RandomHorizontalFlip,RandomCrop\n",
    "from tqdm.notebook import tqdm\n",
    "import torchshow as ts\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from model import *\n",
    "# from utils import setup_seed\n",
    "from models import *\n",
    "from new_poi_util import *\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.set_device(1)\n",
    "set_seed(0)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 512\n",
    "nworkers = 8\n",
    "valid_size = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose([ToTensor(),])\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10('/home/minzhou/data', train=True, download=False, transform=None)\n",
    "val_dataset = torchvision.datasets.CIFAR10('/home/minzhou/data', train=False, download=False, transform=None)\n",
    "\n",
    "test_dataset = torchvision.datasets.CIFAR10('/home/minzhou/data', train=False, download=False, transform=None)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=nworkers, pin_memory=True)\n",
    "\n",
    "val_idx = []\n",
    "for i in range(10):\n",
    "    current_label = np.where(np.array(val_dataset.targets)==i)[0]\n",
    "    samples_idx = np.random.choice(current_label, size=int(valid_size/10), replace=False)\n",
    "    val_idx.extend(samples_idx)\n",
    "\n",
    "val_set = my_subset(val_dataset, val_idx, transform = transform)\n",
    "# val_set = torchvision.datasets.FakeData(size = 1000, image_size = (3, 32, 32), num_classes = 10, transform = tensor_trans)\n",
    "meta_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=nworkers, pin_memory=True)\n",
    "\n",
    "# #Delete the train set from dataset\n",
    "train_poi_set, o_poi_idx = poi_dataset(train_dataset, poi_methond='backdoor', transform=transform, poi_rates=0.05,random_seed=0, tar_lab=2)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_poi_set, batch_size=batch_size, num_workers=nworkers, pin_memory=True, shuffle=True)\n",
    "\n",
    "poi_set = Subset(train_poi_set, o_poi_idx)\n",
    "poi_dataloader = torch.utils.data.DataLoader(poi_set, batch_size=batch_size, num_workers=nworkers, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18()\n",
    "# model.load_state_dict(torch.load('./checkpoint/warmp3_cifar10.pth'))\n",
    "model = model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "optimizer2 = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "full_ce = nn.CrossEntropyLoss(reduction='none')\n",
    "bce = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HiddenLayer(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(HiddenLayer, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.relu(self.fc(x))\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size = 10, hidden_size=100, num_layers=1):\n",
    "        super(MLP, self).__init__()\n",
    "        self.first_hidden_layer = HiddenLayer(input_size, hidden_size)\n",
    "        self.rest_hidden_layers = nn.Sequential(*[HiddenLayer(hidden_size, hidden_size) for _ in range(num_layers - 1)])\n",
    "        self.output_layer = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.first_hidden_layer(x)\n",
    "        x = self.rest_hidden_layers(x)\n",
    "        x = self.output_layer(x)\n",
    "        return torch.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_model = ResNet18()\n",
    "o_model.load_state_dict(torch.load('/home/minzhou/public_html/dataeval/new_first_phase/checkpoint/cifar10_backdoor_0.1_resnet18_tar2.pth'))\n",
    "o_model = o_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o_model.eval()\n",
    "correct_clean, total_clean = 0, 0\n",
    "for i, (images, labels,_) in enumerate(poi_dataloader):\n",
    "    images, labels = images.to(device), labels.to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = o_model(images)\n",
    "        out_loss = criterion(logits,labels)\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total_clean += labels.size(0)\n",
    "        correct_clean += (predicted == labels).sum().item()\n",
    "acc_clean = correct_clean / total_clean\n",
    "print('\\nASR %.2f' % (acc_clean*100))\n",
    "print('Test_loss:',out_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "o_model.eval()\n",
    "correct_clean, total_clean = 0, 0\n",
    "idxs = random.sample(range(valid_size), min(batch_size,valid_size))\n",
    "neg_img = torch.stack([val_set[i][0] for i in idxs]).to(device)\n",
    "neg_lab = torch.tensor([val_set[i][1] for i in idxs]).to(device)\n",
    "with torch.no_grad():\n",
    "    logits = o_model(neg_img)\n",
    "    out_loss = criterion(logits,neg_lab)\n",
    "    _, predicted = torch.max(logits.data, 1)\n",
    "    total_clean += neg_lab.size(0)\n",
    "    correct_clean += (predicted == neg_lab).sum().item()\n",
    "acc_clean = correct_clean / total_clean\n",
    "print('\\nClean ACC %.2f' % (acc_clean*100))\n",
    "print('Test_loss:',out_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in tqdm(range(1)):\n",
    "    o_model2 = copy.deepcopy(o_model)\n",
    "    o_model2.train()\n",
    "    \n",
    "    model_hat = copy.deepcopy(o_model2)\n",
    "    layer_cake = list(model_hat.children())\n",
    "    model_hat = torch.nn.Sequential(*(layer_cake[:-1]), torch.nn.Flatten())\n",
    "    model_hat = model_hat.to(device)\n",
    "    model_hat = model_hat.train()\n",
    "    model.train()\n",
    "    \n",
    "    for iters, (input_train, target_train, poi) in enumerate(train_dataloader):\n",
    "        pos_img,pos_lab,poi = input_train.cuda(), target_train.cuda(), poi.cuda()\n",
    "        idxs = random.sample(range(valid_size), min(batch_size,valid_size))\n",
    "        neg_img = torch.stack([val_set[i][0] for i in idxs]).to(device)\n",
    "        neg_lab = torch.tensor([val_set[i][1] for i in idxs]).to(device)\n",
    "        neg_outputs = model(neg_img)\n",
    "        neg_loss = torch.mean(torch.var(neg_outputs,dim=1))\n",
    "        optimizer.zero_grad()\n",
    "        neg_loss.backward()\n",
    "        optimizer.step()\n",
    "        poi = poi.cuda()\n",
    "            \n",
    "        Vnet = MLP(input_size=8192, hidden_size=128, num_layers=2).to(device)\n",
    "        Vnet.train()\n",
    "        optimizer_hat = torch.optim.Adam(Vnet.parameters(), lr=0.0001)\n",
    "        optimizer_hat2 = torch.optim.Adam(Vnet.parameters(), lr=0.0001)\n",
    "        for _ in range(100):\n",
    "            \n",
    "            v_outputs = model_hat(pos_img)\n",
    "            vneto = Vnet(v_outputs)\n",
    "            v_label = torch.ones(v_outputs.shape[0]).to(device)\n",
    "            rr_loss = bce(vneto.view(-1),v_label)\n",
    "            Vnet.zero_grad()\n",
    "            rr_loss.backward()\n",
    "            optimizer_hat.step()\n",
    "            \n",
    "            vn_outputs = model_hat(neg_img)\n",
    "            v_label2 = torch.zeros(vn_outputs.shape[0]).to(device)\n",
    "            vneto2 = Vnet(vn_outputs)\n",
    "            rr_loss2 = bce(vneto2.view(-1),v_label2)\n",
    "            Vnet.zero_grad()\n",
    "            rr_loss2.backward()\n",
    "            optimizer_hat2.step()\n",
    "\n",
    "        \n",
    "        res = Vnet(v_outputs)\n",
    "        pidx = torch.where(adjusted_outlyingness(res) > 2)[0]\n",
    "        pos_outputs = model(pos_img[pidx])\n",
    "        real_loss = -criterion(pos_outputs, pos_lab[pidx])\n",
    "        optimizer2.zero_grad()\n",
    "        real_loss.backward()\n",
    "        optimizer2.step()\n",
    "        print(neg_loss, real_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(model, dataset, poi_idx):\n",
    "    poi_set = Subset(dataset, poi_idx)\n",
    "    clean_idx = list(set(np.arange(len(dataset))) - set(poi_idx))\n",
    "    clean_set = Subset(dataset,clean_idx)\n",
    "    \n",
    "    poiloader = torch.utils.data.DataLoader(poi_set, batch_size=512, shuffle=False, num_workers=4)\n",
    "    cleanloader = torch.utils.data.DataLoader(clean_set, batch_size=512, shuffle=False, num_workers=4)\n",
    "    \n",
    "    poi_res = []\n",
    "    for i, (data, target,_) in enumerate(tqdm(poiloader)):\n",
    "        data, target= data.to(device), target.to(device)\n",
    "        with torch.no_grad():\n",
    "            poi_outputs = model(data)\n",
    "            # poi_loss = torch.var(poi_outputs,dim=1)\n",
    "            poi_loss = full_ce(poi_outputs, target)\n",
    "            poi_res.extend(poi_loss.cpu().detach().numpy())\n",
    "            \n",
    "    clean_res = []\n",
    "    model.eval()\n",
    "    for i, (data, target,_) in enumerate(tqdm(cleanloader)):\n",
    "        data, target= data.to(device), target.to(device)\n",
    "        with torch.no_grad():\n",
    "            clean_outputs = model(data)\n",
    "            # clean_loss = torch.var(clean_outputs,dim=1)\n",
    "            clean_loss = full_ce(clean_outputs, target)\n",
    "            clean_res.extend(clean_loss.cpu().detach().numpy())\n",
    "            \n",
    "    return poi_res, clean_res\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_res, clean_res = get_result(model, train_poi_set, o_poi_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poi_true = [1 for i in range(len(poi_res))]\n",
    "nor_true = [0 for i in range(len(clean_res))]\n",
    "\n",
    "true_label = poi_true + nor_true\n",
    "pred_label = poi_res + clean_res\n",
    "\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fpr, tpr, thersholds = roc_curve(true_label, pred_label)\n",
    " \n",
    "roc_auc = auc(fpr, tpr)\n",
    "print(roc_auc_score(true_label, pred_label))\n",
    "\n",
    "plt.plot(fpr, tpr, label='ROC (area = {0:.2f})'.format(roc_auc), lw=2)\n",
    "\n",
    " \n",
    "plt.xlim([-0.05, 1.05])\n",
    "plt.ylim([-0.05, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_res(clean_res, poi_res):\n",
    "    plt.figure(figsize=(3,1.5), dpi=300)\n",
    "    plt.hist(np.array(clean_res), bins=200,label='Clean', color=\"#5da1f0\")\n",
    "    plt.hist(np.array(poi_res), bins=200,label='Poison', color=\"#f7d145\")\n",
    "    # plt.axvline(12.71615742,label='Threshold', color=\"green\")\n",
    "\n",
    "    # plt.axvline(20,label='Threshold', color=\"red\", lw=0.8, ls='-.')\n",
    "    plt.ylabel(\"Number of samples\")\n",
    "    # plt.xlabel(\"Result\")\n",
    "    plt.xticks([])\n",
    "    plt.ylim(0, 500)\n",
    "    plt.ticklabel_format(style='sci',scilimits=(0,0),axis='both')\n",
    "    # plt.xlim(0, 40)\n",
    "    plt.legend(prop={'size': 6})\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_res(clean_res, poi_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = poi_res + clean_res\n",
    "t = get_t(total, 1e-6)\n",
    "print(\"tp:\", len(o_poi_idx)-np.where(np.array(poi_res) < t)[0].shape[0])\n",
    "print(\"fp:\", len(clean_res)-np.where(np.array(clean_res) < t)[0].shape[0])\n",
    "print(\"fn:\", np.where(np.array(poi_res) < t)[0].shape[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b8be34f2a64f133f414bd034f75b72cc1c8d29070f6944ffe8bd65ff6cd5b9f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
